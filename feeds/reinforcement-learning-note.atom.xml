<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Atsushi's blog - reinforcement learning, note</title><link href="/" rel="alternate"></link><link href="/feeds/reinforcement-learning-note.atom.xml" rel="self"></link><id>/</id><updated>2021-06-11T00:00:00+09:00</updated><entry><title>Naive Deep Q Learning: Coding Deep Q Network with PyTorch</title><link href="/Naive%20Deep%20Q%20Learning.html" rel="alternate"></link><published>2021-06-11T00:00:00+09:00</published><updated>2021-06-11T00:00:00+09:00</updated><author><name>toohsk</name></author><id>tag:None,2021-06-11:/Naive Deep Q Learning.html</id><summary type="html">&lt;p&gt;Rough note of implementing deep q learning.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Naive Deep Q Learning: Coding Deep Q Network&lt;/h1&gt;
&lt;p&gt;Implement Naive Deep Q Learning code, &lt;a href="https://github.com/toohsk/udemy-deep-q-learning-from-paper-to-code/tree/main/naive_deep_q_learning"&gt;here&lt;/a&gt;.
And the learning result was like in the figure.&lt;/p&gt;
&lt;p&gt;&lt;img alt="image of training result of deep q agent in cartpole environment" src="images/cartpole_naive_dqn.png"&gt;&lt;/p&gt;
&lt;p&gt;As the result, after epsilon tends to minimum value, the agent starts to take actions and learn from it. But when you look closer to the score line, the wild fluctuations in the score became more and more severe after 6000 steps.
Why this happen?&lt;/p&gt;
&lt;h2&gt;Reason of score’s fluctuations&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Learning from a single example.
    The agent  sees many thousands of steps, and this experience is effectively discarded each time.&lt;/li&gt;
&lt;li&gt;Epsilon value
    When epsilon is large, the agent could jump around in the highly dimensional space. But the epsilon slightly getting smaller, that makes the agent only look in the minimum local area.&lt;/li&gt;
&lt;li&gt;Using the same network for evaluate and maximum action
    The agent is choosing the maximum action but it’ll be updated at every step. So the agent will chasing the moving target. The maximum action is role of bias, but getting optimal bias need a lot of cost.&lt;/li&gt;
&lt;/ol&gt;</content><category term="reinforcement learning, note"></category><category term="reinforcement learning"></category></entry><entry><title>Fundamentals of Reinforcement Learning.</title><link href="/ReinforcementLearning.html" rel="alternate"></link><published>2021-06-10T00:00:00+09:00</published><updated>2021-06-10T00:00:00+09:00</updated><author><name>toohsk</name></author><id>tag:None,2021-06-10:/ReinforcementLearning.html</id><summary type="html">&lt;p&gt;Rough note of reinforcement learing.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Rough note of reinforcement learing.&lt;/h1&gt;
&lt;p&gt;This is a note of the Udemy's &lt;a href="https://www.udemy.com/course/deep-q-learning-from-paper-to-code/"&gt;Modern Reinforcement Learning: Deep Q Learning in PyTorch course&lt;/a&gt;, chapter 2.&lt;br&gt;
This note is just for helping my understanding so if you want correct information, please take a look above course.&lt;br&gt;
It is hard and also I'm still in the path of this course but I highly recommend to take this!&lt;/p&gt;
&lt;p&gt;There are a lot of equations and I hand write them, but I won't upload them.&lt;/p&gt;
&lt;p&gt;And records of my studying is uploaded to &lt;a href="https://github.com/toohsk/udemy-deep-q-learning-from-paper-to-code"&gt;GitHub repository&lt;/a&gt;, so please stay tune.&lt;/p&gt;
&lt;h1&gt;Notes&lt;/h1&gt;
&lt;h2&gt;What's Reinforcement Learning?&lt;/h2&gt;
&lt;p&gt;Interactions between agent and environment.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Agent learns and act(decision making)&lt;/li&gt;
&lt;li&gt;Environment gives states and rewards&lt;/li&gt;
&lt;li&gt;Rewards tell the agent what is good and bad&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The biggest difference between Reinforcement learning and Supervised learning is, in the Reinforcement learning agent also makes decisions from the action space based on the current state but it affect all future rewards.&lt;/p&gt;
&lt;p&gt;Actions cause state transitions&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;p(s&amp;#39;, r | a, s) != 1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;s: state
r: reward
a: action&lt;/p&gt;
&lt;p&gt;this equation defines probabilities of given next state (s’) and reward(r) when agent have taken an action (a) in the state (s).&lt;/p&gt;
&lt;p&gt;And summation of probabilities get 1.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Sigma( p(s&amp;#39;, r | a, s) = 1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Total rewards are expression as G, and total rewards in time step t is expressed as&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;G_{t} = R_{t+1} + R_{t+2} + ... + R_{T}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In reinforcement learning, tasks are expected episodic, which it stands for there is final state in the task.
But in some tasks continues to infinity means as forever. And in this case, rewards towards to infinity, too.&lt;br&gt;
To deal with this problem, rewards are fixed by discounting.
Discount factor is expressed with Gamma, γ.
When gamma equals to 1, the agent values all future rewards equally and interests in play long game.
When gamma equals to 0, the agent is interests in the short games, it place no values for all future rewards.&lt;br&gt;
Gamma needs to be in the range of &lt;code&gt;0 &amp;lt;= γ &amp;lt;= 1&lt;/code&gt;, and in the practice it is set in range of &lt;code&gt;0.95 &amp;lt;= γ &amp;lt;= 0.99&lt;/code&gt;.
And the discounted total rewards are expressed as&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;G_{t} = γ^0 * R_{t+1} + γ^1 * R_{t+2} + γ^2 * R_{t+3} + ... + γ^{T-1} * R_{T} = Sigma(γ^k * R_{t+k+1})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Policy is mapping of states to actions.
And it represented as Pi, π.&lt;/p&gt;
&lt;h2&gt;Value Functions, Action Functions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Every policy has a v and q, and those obey the Bellman equation.&lt;/li&gt;
&lt;li&gt;Policies can be ranked with v and q.&lt;ul&gt;
&lt;li&gt;better value is better policy&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Bellman optimality equation is recursive&lt;ul&gt;
&lt;li&gt;v and q functions has next state, s’, and next action, a’&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;About Q Learning&lt;/h2&gt;
&lt;p&gt;Q Learning is based on model free approach.
It is trying to estimate optimal p.&lt;/p&gt;
&lt;p&gt;For estimating optimal p, there are two approaches.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;model based approach&lt;/li&gt;
&lt;li&gt;model free approach&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The model free approach is the way to learn from trial and error, and figure out the best action to get best reward.&lt;/p&gt;
&lt;h2&gt;The Explore-Exploit Dilemma&lt;/h2&gt;
&lt;p&gt;Then How to learn &amp;amp; get maximize rewards?&lt;/p&gt;
&lt;p&gt;Taking the best actions by the agent is known as greed.
And taking sub optimal action is known as exploration.
Balancing the two operations is a dilemma.&lt;/p&gt;
&lt;p&gt;For this solution is bring another parameter, called Epsilon Greedy.
The epsilon greedy is a hyper parameter for selecting greedy actions or exploration actions.
Epsilon would be decreased over time step, so that the agent would take random actions for first, but go through the steps greedy actions would be more selected by decreasing it.
Epsilon must stay finite.
For practice, epsilon settle on &lt;code&gt;0.01 &amp;lt;= ε &amp;lt;= 0.1&lt;/code&gt;, and this means the agent spends from 1% to 10% chance to take exploratory actions.&lt;/p&gt;
&lt;h2&gt;Temporal Difference Learning&lt;/h2&gt;
&lt;p&gt;Estimate of value of the policy and needs to refine it.
Temporal Difference Learning is online learning.
At the boot strap using one estimate to update another.&lt;/p&gt;
&lt;h2&gt;Q Learning Algorithm&lt;/h2&gt;
&lt;p&gt;The Q Learning is a tabular and off policy method.&lt;/p&gt;
&lt;p&gt;Basic algorithm is shown as below.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initialize Q for all states s and action a&lt;/li&gt;
&lt;li&gt;Initialize parameters: alpha=0.001, gamma=0.9, epsilon_max=1.0, epsilon_min=0.01&lt;/li&gt;
&lt;li&gt;Repeat for n_episodes&lt;ol&gt;
&lt;li&gt;Initialize state s&lt;/li&gt;
&lt;li&gt;For each step of episode&lt;ol&gt;
&lt;li&gt;Choose action a with epsilon-greedy&lt;/li&gt;
&lt;li&gt;Perform action a, get new state s’ and reward r&lt;/li&gt;
&lt;li&gt;Apply Q function and update Q(s,a)&lt;/li&gt;
&lt;li&gt;update status s to s’ by s=s’&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Point to implement Q Learning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Agent should be a class&lt;ul&gt;
&lt;li&gt;Class has&lt;ul&gt;
&lt;li&gt;Q table&lt;/li&gt;
&lt;li&gt;Learning rate&lt;/li&gt;
&lt;li&gt;discount factor&lt;/li&gt;
&lt;li&gt;epsilon&lt;/li&gt;
&lt;li&gt;initializing function the Q table&lt;/li&gt;
&lt;li&gt;choosing action function based on current state&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Represent the Q table as a dictionary.&lt;ul&gt;
&lt;li&gt;states and actions as the key&lt;/li&gt;
&lt;li&gt;reward would be value&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Decrease epsilon over time to minimum value.&lt;/li&gt;
&lt;/ul&gt;</content><category term="reinforcement learning, note"></category><category term="reinforcement learning"></category></entry></feed>